{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.combine import SMOTEENN\n",
        "from azureml.core import Workspace, Dataset\n",
        "\n",
        "# Load the dataset\n",
        "ws = Workspace.from_config()\n",
        "datastore = ws.get_default_datastore()\n",
        "dataset = Dataset.Tabular.from_delimited_files(path=(datastore, '/olympic_medals.csv'), infer_column_types=False, validate=False)\n",
        "df = dataset.to_pandas_dataframe(infer_column_types=False)\n",
        "\n",
        "# file_path = 'olympic_medals.csv'\n",
        "# df = pd.read_csv(file_path)\n",
        "\n",
        "# Filter for Summer Olympics and the years 2004, 2008, 2012, 2016, and 2020\n",
        "summer_years = ['athens-2004', 'beijing-2008', 'london-2012', 'rio-2016', 'tokyo-2020']\n",
        "df_summer = df[df['slug_game'].isin(summer_years)].copy()\n",
        "\n",
        "# Extract the year from the 'slug_game' column and create a new 'year' column\n",
        "df_summer['year'] = df_summer['slug_game'].str.extract(r'(\\d{4})').astype(int)\n",
        "\n",
        "# Create a pivot table to count the medals by country, discipline, year, and gender\n",
        "medal_counts = df_summer.pivot_table(index=['country_name', 'discipline_title', 'year', 'event_gender'],\n",
        "                                     columns='medal_type',\n",
        "                                     aggfunc='size',\n",
        "                                     fill_value=0).reset_index()\n",
        "\n",
        "# Calculate the total number of medals\n",
        "medal_counts['total'] = medal_counts[['GOLD', 'SILVER', 'BRONZE']].sum(axis=1)\n",
        "\n",
        "# Rename columns for clarity\n",
        "medal_counts.columns = ['country', 'discipline', 'year', 'event_gender', 'gold', 'silver', 'bronze', 'total']\n",
        "\n",
        "# Prepare features and target\n",
        "X = medal_counts[['year', 'gold', 'silver', 'bronze', 'event_gender']].copy()\n",
        "\n",
        "# Convert gender to numeric values for the model\n",
        "event_gender_mapping = {'Men': 1, 'Women': 0, 'Mixed': 2}\n",
        "X['event_gender'] = X['event_gender'].map(event_gender_mapping)\n",
        "\n",
        "y_gold = medal_counts['gold']\n",
        "y_silver = medal_counts['silver']\n",
        "y_bronze = medal_counts['bronze']\n",
        "y_total = medal_counts['total']\n",
        "\n",
        "# Handle missing values\n",
        "X = X.fillna(X.mean()).infer_objects(copy=False)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X[['year', 'gold', 'silver', 'bronze']] = scaler.fit_transform(X[['year', 'gold', 'silver', 'bronze']])\n",
        "\n",
        "# Function to train and predict medals\n",
        "def train_and_predict(y, medal_type):\n",
        "    # Check the distribution of the target variable\n",
        "    print(y.value_counts())\n",
        "\n",
        "    # Filter out classes with fewer than 2 samples\n",
        "    valid_classes = y.value_counts()[y.value_counts() > 1].index\n",
        "    valid_indices = y.isin(valid_classes)\n",
        "    X_valid = X[valid_indices].copy()\n",
        "    y_valid = y[valid_indices]\n",
        "\n",
        "    # Handle imbalanced data using SMOTEENN\n",
        "    min_samples = min(y_valid.value_counts())\n",
        "    k_neighbors = max(1, min_samples - 1)\n",
        "    print(f\"Using k_neighbors={k_neighbors} for SMOTE\")\n",
        "\n",
        "    smote = SMOTE(k_neighbors=k_neighbors)\n",
        "    smote_enn = SMOTEENN(random_state=42, smote=smote)\n",
        "    X_resampled, y_resampled = smote_enn.fit_resample(X_valid, y_valid)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train the RandomForest model\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict future medals for the next Olympics\n",
        "    next_olympics_year = 2024\n",
        "    latest_year_data = medal_counts[medal_counts['year'] == medal_counts['year'].max()].copy()\n",
        "    latest_year_data['year'] = next_olympics_year  # Update the year\n",
        "\n",
        "    # Map gender values to numeric for the prediction\n",
        "    latest_year_data['event_gender'] = latest_year_data['event_gender'].map(event_gender_mapping)\n",
        "\n",
        "    # Normalize the prediction data\n",
        "    latest_year_data[['year', 'gold', 'silver', 'bronze']] = scaler.transform(latest_year_data[['year', 'gold', 'silver', 'bronze']])\n",
        "\n",
        "    future_predictions = model.predict(latest_year_data[['year', 'gold', 'silver', 'bronze', 'event_gender']])\n",
        "\n",
        "    latest_year_data[f'Predicted_{medal_type}'] = future_predictions\n",
        "\n",
        "    return latest_year_data[['country', 'discipline', 'event_gender', f'Predicted_{medal_type}']]\n",
        "\n",
        "# Predict gold, silver, and bronze medals\n",
        "predicted_gold = train_and_predict(y_gold, 'Gold')\n",
        "predicted_silver = train_and_predict(y_silver, 'Silver')\n",
        "predicted_bronze = train_and_predict(y_bronze, 'Bronze')\n",
        "\n",
        "# Combine predictions\n",
        "predicted_data = pd.merge(predicted_gold, predicted_silver, on=['country', 'discipline', 'event_gender'])\n",
        "predicted_data = pd.merge(predicted_data, predicted_bronze, on=['country', 'discipline', 'event_gender'])\n",
        "\n",
        "# Convert gender back to categorical for better visualization\n",
        "event_gender_mapping_reverse = {1: 'Men', 0: 'Women', 2: 'Mixed'}\n",
        "predicted_data['event_gender'] = predicted_data['event_gender'].map(event_gender_mapping_reverse)\n",
        "\n",
        "# Plot the data\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=predicted_data.groupby('country')['Predicted_Gold'].sum().reset_index(), x='country', y='Predicted_Gold')\n",
        "plt.title('Predicted Gold Medals by Country')\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Predicted Gold Medals')\n",
        "plt.xlabel('Country')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=predicted_data.groupby('country')['Predicted_Silver'].sum().reset_index(), x='country', y='Predicted_Silver')\n",
        "plt.title('Predicted Silver Medals by Country')\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Predicted Silver Medals')\n",
        "plt.xlabel('Country')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=predicted_data.groupby('country')['Predicted_Bronze'].sum().reset_index(), x='country', y='Predicted_Bronze')\n",
        "plt.title('Predicted Bronze Medals by Country')\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Predicted Bronze Medals')\n",
        "plt.xlabel('Country')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "predicted_data.to_csv('predicted_olympic_medals.csv', index=False)\n",
        "print('Predicted medals saved to predicted_olympic_medals.csv')\n",
        "\n",
        "# Predict total medals\n",
        "def train_and_predict_total():\n",
        "    # Check the distribution of the target variable\n",
        "    print(y_total.value_counts())\n",
        "\n",
        "    # Filter out classes with fewer than 2 samples\n",
        "    valid_classes = y_total.value_counts()[y_total.value_counts() > 1].index\n",
        "    valid_indices = y_total.isin(valid_classes)\n",
        "    X_valid = X[valid_indices].copy()\n",
        "    y_valid = y_total[valid_indices]\n",
        "\n",
        "    # Handle imbalanced data using SMOTEENN\n",
        "    min_samples = min(y_valid.value_counts())\n",
        "    k_neighbors = max(1, min_samples - 1)\n",
        "    print(f\"Using k_neighbors={k_neighbors} for SMOTE\")\n",
        "\n",
        "    smote = SMOTE(k_neighbors=k_neighbors)\n",
        "    smote_enn = SMOTEENN(random_state=42, smote=smote)\n",
        "    X_resampled, y_resampled = smote_enn.fit_resample(X_valid, y_valid)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train the RandomForest model\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict future medals for the next Olympics\n",
        "    next_olympics_year = 2024\n",
        "    latest_year_data = medal_counts[medal_counts['year'] == medal_counts['year'].max()].copy()\n",
        "    latest_year_data['year'] = next_olympics_year  # Update the year\n",
        "\n",
        "    # Map gender values to numeric for the prediction\n",
        "    latest_year_data['event_gender'] = latest_year_data['event_gender'].map(event_gender_mapping)\n",
        "\n",
        "    # Normalize the prediction data\n",
        "    latest_year_data[['year', 'gold', 'silver', 'bronze']] = scaler.transform(latest_year_data[['year', 'gold', 'silver', 'bronze']])\n",
        "\n",
        "    future_predictions = model.predict(latest_year_data[['year', 'gold', 'silver', 'bronze', 'event_gender']])\n",
        "\n",
        "    # Round the predictions to the nearest integer\n",
        "    latest_year_data['Predicted_Total'] = future_predictions.round().astype(int)\n",
        "\n",
        "    return latest_year_data[['country', 'Predicted_Total']]\n",
        "\n",
        "# Predict total medals\n",
        "predicted_total = train_and_predict_total()\n",
        "\n",
        "# Aggregate the predicted totals by country\n",
        "predicted_total_by_country = predicted_total.groupby('country')['Predicted_Total'].sum().reset_index()\n",
        "\n",
        "# Plot the data\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=predicted_total_by_country, x='country', y='Predicted_Total')\n",
        "plt.title('Predicted Total Medals by Country')\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Predicted Total Medals')\n",
        "plt.xlabel('Country')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "predicted_total_by_country.to_csv('predicted_total_medals.csv', index=False)\n",
        "print('Predicted total medals saved to predicted_total_medals.csv')\n",
        "\n",
        "# Display the first few rows of the final data to verify the output\n",
        "print(predicted_total_by_country.head())\n",
        "\n",
        "# Function to show medals (gold, silver, bronze) of a selected country\n",
        "def show_country_medals(country):\n",
        "    country_data = predicted_data[predicted_data['country'] == country].groupby('country').sum().reset_index()\n",
        "    if country_data.empty:\n",
        "        print(f\"No data available for {country}.\")\n",
        "        return\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.barplot(x=['Gold', 'Silver', 'Bronze'], y=[country_data['Predicted_Gold'][0], country_data['Predicted_Silver'][0], country_data['Predicted_Bronze'][0]])\n",
        "    plt.title(f'Predicted Medals for {country}')\n",
        "    plt.ylabel('Number of Medals')\n",
        "    plt.xlabel('Medal Type')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Get input from user for country and show medals\n",
        "selected_country = input(\"Enter the country name: \")\n",
        "show_country_medals(selected_country)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2024-07-08 16:41:09.426789 | ActivityCompleted: Activity=from_delimited_files, HowEnded=Failure, Duration=144.38 [ms], Info = {'activity_id': '1b40856c-a2dd-42ec-8678-5db4eac834fe', 'activity_name': 'from_delimited_files', 'activity_type': 'PublicApi', 'app_name': 'TabularDataset', 'source': 'azureml.dataset', 'version': '1.56.0', 'dataprepVersion': '5.1.6', 'sparkVersion': '', 'subscription': '', 'run_id': '', 'resource_group': '', 'workspace_name': '', 'experiment_id': '', 'location': '', 'completionStatus': 'Success', 'durationMs': 0.07}, Exception=DatasetValidationError; DatasetValidationError:\n\tMessage: Failed to validate the data.If data is inaccessible, please set validate to False.\nThe requested stream was not found. Please make sure the request uri is correct.| session_id=l_71966fb7-ec71-4743-83d8-8d7b45b31b99\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Failed to validate the data.If data is inaccessible, please set validate to False.\\nThe requested stream was not found. Please make sure the request uri is correct.| session_id=l_71966fb7-ec71-4743-83d8-8d7b45b31b99\"\n    }\n}\n"
        },
        {
          "output_type": "error",
          "ename": "DatasetValidationError",
          "evalue": "DatasetValidationError:\n\tMessage: Failed to validate the data.If data is inaccessible, please set validate to False.\nThe requested stream was not found. Please make sure the request uri is correct.| session_id=l_71966fb7-ec71-4743-83d8-8d7b45b31b99\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Failed to validate the data.If data is inaccessible, please set validate to False.\\nThe requested stream was not found. Please make sure the request uri is correct.| session_id=l_71966fb7-ec71-4743-83d8-8d7b45b31b99\"\n    }\n}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mExecutionError\u001b[0m                            Traceback (most recent call last)",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azureml/data/dataset_error_handling.py:66\u001b[0m, in \u001b[0;36m_validate_has_data\u001b[0;34m(dataflow, error_message)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[43mdataflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify_has_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (dataprep()\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mdataflow\u001b[38;5;241m.\u001b[39mDataflowValidationError,\n\u001b[1;32m     68\u001b[0m         dataprep()\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39merrorhandlers\u001b[38;5;241m.\u001b[39mExecutionError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azureml/dataprep/api/_loggerfactory.py:279\u001b[0m, in \u001b[0;36mtrack.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azureml/dataprep/api/engineless_dataflow.py:1411\u001b[0m, in \u001b[0;36mEnginelessDataflow.verify_has_data\u001b[0;34m(self, verify_columns)\u001b[0m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39mstart_as_current_span(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnginelessDataflow.verify_has_data\u001b[39m\u001b[38;5;124m'\u001b[39m, trace\u001b[38;5;241m.\u001b[39mget_current_span()):\n\u001b[0;32m-> 1411\u001b[0m       records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_pyrecords\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_new_flow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1412\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(records) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azureml/dataprep/api/_loggerfactory.py:279\u001b[0m, in \u001b[0;36mtrack.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azureml/dataprep/api/engineless_dataflow.py:1426\u001b[0m, in \u001b[0;36mEnginelessDataflow._to_pyrecords\u001b[0;34m(self, use_new_flow)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39mstart_as_current_span(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnginelessDataflow._to_pyrecords\u001b[39m\u001b[38;5;124m'\u001b[39m, trace\u001b[38;5;241m.\u001b[39mget_current_span()) \u001b[38;5;28;01mas\u001b[39;00m span:\n\u001b[0;32m-> 1426\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pyrecords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_yaml_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_dprep_span_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspan_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1427\u001b[0m     record_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(records)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azureml/dataprep/api/_rslex_executor.py:41\u001b[0m, in \u001b[0;36m_RsLexExecutor.to_pyrecords\u001b[0;34m(self, script, traceparent)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazureml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataprep\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrslex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Executor\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_dataflow_to_pyrecords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceparent\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mExecutionError\u001b[0m: \nError Code: ScriptExecution.StreamAccess.NotFound\nNative Error: Dataflow visit error: ExecutionError(StreamError(NotFound))\n\tVisitError(ExecutionError(StreamError(NotFound)))\n=> Failed with execution error: error in streaming from input data sources\n\tExecutionError(StreamError(NotFound))\nError Message: The requested stream was not found. Please make sure the request uri is correct.| session_id=l_71966fb7-ec71-4743-83d8-8d7b45b31b99",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mDatasetValidationError\u001b[0m                    Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m ws \u001b[38;5;241m=\u001b[39m Workspace\u001b[38;5;241m.\u001b[39mfrom_config()\n\u001b[1;32m     13\u001b[0m datastore \u001b[38;5;241m=\u001b[39m ws\u001b[38;5;241m.\u001b[39mget_default_datastore()\n\u001b[0;32m---> 14\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTabular\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_delimited_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatastore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath/to/olympic_medals.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_column_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m df \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mto_pandas_dataframe(infer_column_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# file_path = 'olympic_medals.csv'\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# df = pd.read_csv(file_path)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Filter for Summer Olympics and the years 2004, 2008, 2012, 2016, and 2020\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azureml/data/_loggerfactory.py:140\u001b[0m, in \u001b[0;36mtrack.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _LoggerFactory\u001b[38;5;241m.\u001b[39mtrack_activity(logger, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions) \u001b[38;5;28;01mas\u001b[39;00m al:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(al, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivity_info\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror_code\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azureml/data/dataset_factory.py:350\u001b[0m, in \u001b[0;36mTabularDatasetFactory.from_delimited_files\u001b[0;34m(path, validate, include_path, infer_column_types, set_column_types, separator, header, partition_format, support_multi_line, empty_as_string, encoding)\u001b[0m\n\u001b[1;32m    339\u001b[0m _trace_dataset_creation(path)\n\u001b[1;32m    340\u001b[0m dataflow \u001b[38;5;241m=\u001b[39m dataprep()\u001b[38;5;241m.\u001b[39mread_csv(path,\n\u001b[1;32m    341\u001b[0m                                verify_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    342\u001b[0m                                include_path\u001b[38;5;241m=\u001b[39minclude_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m                                empty_as_string\u001b[38;5;241m=\u001b[39mempty_as_string,\n\u001b[1;32m    348\u001b[0m                                encoding\u001b[38;5;241m=\u001b[39mencoding)\n\u001b[0;32m--> 350\u001b[0m dataflow \u001b[38;5;241m=\u001b[39m \u001b[43m_transform_and_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataflow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataflow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_column_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_column_types\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_is_inference_required\u001b[49m\u001b[43m(\u001b[49m\u001b[43mset_column_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m infer_column_types:\n\u001b[1;32m    356\u001b[0m     column_types_builder \u001b[38;5;241m=\u001b[39m dataflow\u001b[38;5;241m.\u001b[39mbuilders\u001b[38;5;241m.\u001b[39mset_column_types()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azureml/data/dataset_factory.py:1189\u001b[0m, in \u001b[0;36m_transform_and_validate\u001b[0;34m(dataflow, partition_format, validate, infer_column_types)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     dataflow \u001b[38;5;241m=\u001b[39m dataflow\u001b[38;5;241m.\u001b[39m_add_columns_from_partition_format(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPath\u001b[39m\u001b[38;5;124m'\u001b[39m, partition_format, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate:\n\u001b[0;32m-> 1189\u001b[0m     \u001b[43m_validate_has_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFailed to validate the data.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIf data is inaccessible, please set validate to False.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m infer_column_types:\n\u001b[1;32m   1192\u001b[0m     _validate_has_data(dataflow, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to infer column type.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1193\u001b[0m                                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf data is inaccessible, please turn off inference.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azureml/data/dataset_error_handling.py:69\u001b[0m, in \u001b[0;36m_validate_has_data\u001b[0;34m(dataflow, error_message)\u001b[0m\n\u001b[1;32m     66\u001b[0m     dataflow\u001b[38;5;241m.\u001b[39mverify_has_data()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (dataprep()\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mdataflow\u001b[38;5;241m.\u001b[39mDataflowValidationError,\n\u001b[1;32m     68\u001b[0m         dataprep()\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39merrorhandlers\u001b[38;5;241m.\u001b[39mExecutionError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetValidationError(error_message \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m e\u001b[38;5;241m.\u001b[39mcompliant_message, exception\u001b[38;5;241m=\u001b[39me)\n",
            "\u001b[0;31mDatasetValidationError\u001b[0m: DatasetValidationError:\n\tMessage: Failed to validate the data.If data is inaccessible, please set validate to False.\nThe requested stream was not found. Please make sure the request uri is correct.| session_id=l_71966fb7-ec71-4743-83d8-8d7b45b31b99\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Failed to validate the data.If data is inaccessible, please set validate to False.\\nThe requested stream was not found. Please make sure the request uri is correct.| session_id=l_71966fb7-ec71-4743-83d8-8d7b45b31b99\"\n    }\n}"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1720456872121
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}